import modal

from .db import DEFAULT_LLM_SERVER_CONFIGS
from .resources import app, hf_secret, results_volume
from .llm_server import llm_server


MAX_SECONDS_PER_BENCHMARK = 120  # 2 minutes
RESULTS_PATH = "/results"
SCALEDOWN_WINDOW = 5  # 5 seconds
TIMEOUT = 30 * 60  # 30 minutes

benchmarking_image = (
    modal.Image.debian_slim()
    .apt_install("git")
    .pip_install(
        "git+https://github.com/neuralmagic/guidellm.git@55c65c4",
        "prometheus-client",
        "SQLAlchemy",
        "tiktoken",
    )
)

with benchmarking_image.imports():
    from typing import Any, Mapping, Optional
    import asyncio
    import time

    from guidellm.backend import Backend
    from guidellm.executor import Executor
    from guidellm.request import EmulatedRequestGenerator
    from guidellm.request.base import RequestGenerator


def benchmark_runner_cls(region: str):
    def decorator(cls):
        return app.cls(
            image=benchmarking_image,
            secrets=[hf_secret],
            volumes={RESULTS_PATH: results_volume},
            cpu=4,
            memory=2048,
            scaledown_window=SCALEDOWN_WINDOW,
            timeout=TIMEOUT,
            region=region,
        )(cls)

    return decorator


async def _run_executor_for_result(executor):
    report = None

    async for result in executor.run():
        if result.completed:
            report = result.report
            break

    if not report:
        raise ValueError("No report generated by executor")

    return report


class BenchmarkRunner:
    @modal.method()
    def run_benchmark(
        self,
        llm_server_type: str,
        model: str,
        rate_type: str,
        data: str,
        gpu: str,
        server_region: str,
        llm_server_config: Optional[Mapping[str, Any]] = None,
        rate: Optional[float] = None,
        **kwargs,
    ):
        """Benchmarks a LLM deployment on Modal.

        Args:
            llm_server_type (str): The server to use for benchmarking, either
                'vllm' or 'trtllm'.
            model (str): Name of the model to benchmark.
            rate_type (str): The type of rate to use for benchmarking, either
                'constant', 'synchronous', or 'throughput'.
            data (str): A configuration for emulated data (e.g.:
                'prompt_tokens=128,generated_tokens=128').
            gpu (str): The GPU to use for benchmarking.
            server_region (str): Region to run the LLM server on.
            llm_server_config (dict): Configuration for the LLM server.
            rate (float): If rate_type is 'constant', optionally specify the
                number of requests that should be made per second.
        """

        if llm_server_type not in DEFAULT_LLM_SERVER_CONFIGS:
            raise ValueError(
                f"Invalid value for llm_server: {llm_server_type}. Must be one of {DEFAULT_LLM_SERVER_CONFIGS.keys()}"
            )
        elif llm_server_config is None:
            llm_server_config = DEFAULT_LLM_SERVER_CONFIGS[llm_server_type]

        # Start LLM server in background
        with llm_server(
            llm_server_type,
            model=model,
            gpu=gpu,
            region=server_region,
            server_config=llm_server_config,
        ) as (llm_server_url, extra_query):
            # Create backend
            backend_inst = Backend.create(
                backend_type="openai_server",
                target=f"{llm_server_url}/v1",
                model=model,
                extra_query=extra_query,
            )

            request_generator: RequestGenerator

            # Create tokenizer and request generator
            try:
                tokenizer_inst = backend_inst.model_tokenizer()
            except Exception as err:
                raise ValueError("Could not load model's tokenizer") from err

            retries = 0
            while True:
                try:
                    request_generator = EmulatedRequestGenerator(
                        config=data, tokenizer=tokenizer_inst
                    )
                    break
                except Exception as err:
                    print(f"Error creating request generator: {err}")
                    retries += 1
                    time.sleep(min(2**retries, 60))

            # Create executor
            executor = Executor(
                backend=backend_inst,
                request_generator=request_generator,
                mode=rate_type,
                rate=rate if rate_type in ("constant", "poisson") else None,
                max_duration=MAX_SECONDS_PER_BENCHMARK,
            )

            # Run executor
            print(
                "Running executor with args: {}",
                {
                    "mode": rate_type,
                    "rate": rate,
                },
            )

            report = asyncio.run(_run_executor_for_result(executor))
            return report.benchmarks[0].model_dump()


@benchmark_runner_cls(region="us-ashburn-1")
class BenchmarkRunner_OCI_USASHBURN1(BenchmarkRunner):
    pass


@benchmark_runner_cls(region="us-chicago-1")
class BenchmarkRunner_OCI_USCHICAGO1(BenchmarkRunner):
    pass


@benchmark_runner_cls(region="us-east-1")
class BenchmarkRunner_AWS_USEAST1(BenchmarkRunner):
    pass


@benchmark_runner_cls(region="us-east4")
class BenchmarkRunner_GCP_USEAST4(BenchmarkRunner):
    pass


all_benchmark_runner_classes = {
    "us-ashburn-1": BenchmarkRunner_OCI_USASHBURN1,
    "us-east-1": BenchmarkRunner_AWS_USEAST1,
    "us-east4": BenchmarkRunner_GCP_USEAST4,
    "us-chicago-1": BenchmarkRunner_OCI_USCHICAGO1,
}
