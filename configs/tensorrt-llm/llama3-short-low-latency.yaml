id: tensorrt-llm-llama3-short-low-latency
base_config:
  region: us-chicago-1
  llm_server_type: tensorrt-llm
  data:
    - prompt_tokens=128,output_tokens=1024
    - prompt_tokens=1024,output_tokens=128
    - prompt_tokens=512,output_tokens=512
  model: meta-llama/Llama-3.1-8B-Instruct
  gpu: H100
configs:
  - llm_server_config:
      llm_kwargs:
        quant_config:
          quant_algo: FP8

        calib_config:
          calib_batches: 512
          calib_batch_size: 1
          calib_max_seq_length: 2048
          tokenizer_max_seq_length: 4096

        build_config:
          plugin_config:
            multiple_profiles: true
            paged_kv_cache: true
            low_latency_gemm_swiglu_plugin: fp8
            low_latency_gemm_plugin: fp8
          speculative_decoding_mode: LOOKAHEAD_DECODING
          max_input_len: 1024
          max_num_tokens: 1152
          max_batch_size: 1

        speculative_config:
          decoding_type: Lookahead
          max_window_size: 8
          max_ngram_size: 6
          max_verification_set_size: 8
