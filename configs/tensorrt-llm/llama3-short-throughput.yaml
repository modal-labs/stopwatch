id: tensorrt-llm-llama3-short-throughput
base_config:
  region: us-chicago-1
  llm_server_type: tensorrt-llm
  data:
    - prompt_tokens=128,output_tokens=1024
    - prompt_tokens=1024,output_tokens=128
    - prompt_tokens=512,output_tokens=512
configs:
  - model: meta-llama/Llama-3.1-8B-Instruct
    gpu: H100
    llm_server_config:
      llm_kwargs:
        quant_config:
          quant_algo: FP8

        calib_config:
          calib_batches: 512
          calib_batch_size: 1
          calib_max_seq_length: 2048
          tokenizer_max_seq_length: 4096

        build_config:
          plugin_config:
            paged_kv_cache: true
            use_fp8_context_fmha: true
          max_input_len: 1024
          max_num_tokens: 524288
          max_batch_size: 512

  # https://nvidia.github.io/TensorRT-LLM/performance/performance-tuning-guide/fp8-quantization.html
  - model: meta-llama/Llama-3.1-8B-Instruct
    gpu: H100
    llm_server_config:
      llm_kwargs:
        quant_config:
          quant_algo: FP8
          kv_cache_quant_algo: FP8

        calib_config:
          calib_batches: 512
          calib_batch_size: 1
          calib_max_seq_length: 2048
          tokenizer_max_seq_length: 4096

        build_config:
          plugin_config:
            paged_kv_cache: true
            gemm_swiglu_plugin: fp8
            multiple_profiles: true
            # # guide recommends these, but they are for multi-GPU inference
            # reduce_fusion: true
            # user_buffer: true
          max_input_len: 1024
          max_num_tokens: 524288
          max_batch_size: 512
