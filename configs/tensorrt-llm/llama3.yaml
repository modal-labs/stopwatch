id: tensorrt-llm-llama3
base_config:
  region: us-chicago-1
  llm_server_type: tensorrt-llm
  data:
    - prompt_tokens=128,output_tokens=1024
    - prompt_tokens=1024,output_tokens=128
    - prompt_tokens=512,output_tokens=512
    - prompt_tokens=256,output_tokens=2048
    - prompt_tokens=2048,output_tokens=256
    - prompt_tokens=1024,output_tokens=1024
    - prompt_tokens=512,output_tokens=4096
    - prompt_tokens=4096,output_tokens=512
    - prompt_tokens=2048,output_tokens=2048
configs:
  - model: meta-llama/Llama-3.1-8B-Instruct
    gpu: H100!
    llm_server_config:
      - {}

      # Tuned for latency
      - llm_kwargs:
          quant_config:
            quant_algo: FP8

          calib_config:
            calib_batches: 512
            calib_batch_size: 1
            calib_max_seq_length: 2048
            tokenizer_max_seq_length: 4096

          build_config:
            plugin_config:
              multiple_profiles: true
              paged_kv_cache: true
              low_latency_gemm_swiglu_plugin: fp8
              low_latency_gemm_plugin: fp8
            speculative_decoding_mode: LOOKAHEAD_DECODING
            max_input_len: 1024
            max_num_tokens: 8192
            max_batch_size: 1

          speculative_config:
            decoding_type: Lookahead
            max_window_size: 8
            max_ngram_size: 6
            max_verification_set_size: 8

        extra_args: ["--max_batch_size", "1"]

      - llm_kwargs:
          quant_config:
            quant_algo: FP8

          calib_config:
            calib_batches: 512
            calib_batch_size: 1
            calib_max_seq_length: 2048
            tokenizer_max_seq_length: 4096

          build_config:
            plugin_config:
              multiple_profiles: true
              paged_kv_cache: true
              low_latency_gemm_swiglu_plugin: fp8
              low_latency_gemm_plugin: fp8
            speculative_decoding_mode: LOOKAHEAD_DECODING
            max_input_len: 1024
            max_num_tokens: 8192
            max_batch_size: 2

          speculative_config:
            decoding_type: Lookahead
            max_window_size: 8
            max_ngram_size: 6
            max_verification_set_size: 8

        extra_args: ["--max_batch_size", "2"]

      # Tuned for throughput
      - llm_kwargs:
          quant_config:
            quant_algo: FP8

          calib_config:
            calib_batches: 512
            calib_batch_size: 1
            calib_max_seq_length: 2048
            tokenizer_max_seq_length: 4096

          build_config:
            plugin_config:
              paged_kv_cache: true
              use_fp8_context_fmha: true
            max_input_len: 1024
            max_num_tokens: 524288
            max_batch_size: 512

        extra_args: ["--max_batch_size", "512"]

      # Tuned for throughput
      - llm_kwargs:
          quant_config:
            quant_algo: FP8
            kv_cache_quant_algo: FP8

          calib_config:
            calib_batches: 512
            calib_batch_size: 1
            calib_max_seq_length: 2048
            tokenizer_max_seq_length: 4096

          build_config:
            plugin_config:
              paged_kv_cache: true
              gemm_swiglu_plugin: fp8
              multiple_profiles: true
            max_input_len: 1024
            max_num_tokens: 524288
            max_batch_size: 512

        extra_args: ["--max_batch_size", "512"]

  - model: meta-llama/Llama-3.1-70B-Instruct
    gpu: H100!:8
    llm_server_config:
      - {}

      # Tuned for latency
      - llm_kwargs:
          quant_config:
            quant_algo: FP8

          calib_config:
            calib_batches: 512
            calib_batch_size: 1
            calib_max_seq_length: 2048
            tokenizer_max_seq_length: 4096

          build_config:
            plugin_config:
              multiple_profiles: true
              paged_kv_cache: true
              low_latency_gemm_swiglu_plugin: fp8
              low_latency_gemm_plugin: fp8
            speculative_decoding_mode: LOOKAHEAD_DECODING
            max_input_len: 1024
            max_num_tokens: 8192
            max_batch_size: 1

          speculative_config:
            decoding_type: Lookahead
            max_window_size: 8
            max_ngram_size: 6
            max_verification_set_size: 8

        extra_args: ["--max_batch_size", "1"]

      # Tuned for throughput
      - llm_kwargs:
          quant_config:
            quant_algo: FP8

          calib_config:
            calib_batches: 512
            calib_batch_size: 1
            calib_max_seq_length: 2048
            tokenizer_max_seq_length: 4096

          build_config:
            plugin_config:
              paged_kv_cache: true
              use_fp8_context_fmha: true
            max_input_len: 1024
            max_num_tokens: 524288
            max_batch_size: 512

        extra_args: ["--max_batch_size", "512"]

      # Tuned for throughput
      - llm_kwargs:
          quant_config:
            quant_algo: FP8
            kv_cache_quant_algo: FP8

          calib_config:
            calib_batches: 512
            calib_batch_size: 1
            calib_max_seq_length: 2048
            tokenizer_max_seq_length: 4096

          build_config:
            plugin_config:
              paged_kv_cache: true
              gemm_swiglu_plugin: fp8
              multiple_profiles: true
              reduce_fusion: true
              user_buffer: true
            max_input_len: 1024
            max_num_tokens: 524288
            max_batch_size: 512

        extra_args: ["--max_batch_size", "512"]
