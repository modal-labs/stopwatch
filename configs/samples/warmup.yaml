id: warmup
base_config:
  model: meta-llama/Llama-3.2-1B-Instruct
  llm_server_type: sglang
  gpu: H100!
  data: prompt_tokens=128,output_tokens=128
  region: us-east-1
