# This sample configuration file will run four full benchmarks by combining all
# possible options for the options that vary (data and llm_server_type). These
# benchmarks are as follows:
# - 128 prompt tokens + 1024 output tokens with vLLM
# - 1024 prompt tokens + 128 output tokens with vLLM
# - 128 prompt tokens + 1024 output tokens with SGLang
# - 1024 prompt tokens + 128 output tokens with SGLang

id: llama3-8b
base_config:
  model: meta-llama/Llama-3.1-8B-Instruct
  region: us-chicago-1
  llm_server_type:
    - sglang
    - vllm
  gpu: H100!
  data:
    - prompt_tokens=128,output_tokens=1024
    - prompt_tokens=1024,output_tokens=128
