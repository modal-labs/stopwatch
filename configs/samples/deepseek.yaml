id: deepseek
base_config:
  model: deepseek-ai/DeepSeek-V3-0324
  data: prompt_tokens=1024,output_tokens=128
  llm_server_type: vllm
  llm_server_config:
    extra_args: ["--tensor-parallel-size", "8", "--max-model-len", "16384"]
configs:
  - gpu: H200:8
    region: us-east-1
  - gpu: B200:8
    region: us-ashburn-1
