id: tensorrt-llm
repeats: 3
base_config:
  region: us-chicago-1
  llm_server_type: tensorrt-llm
  data:
    - prompt_tokens=256,generated_tokens=4096
    - prompt_tokens=4096,generated_tokens=256
    - prompt_tokens=2048,generated_tokens=2048
configs:
  - model: zed-industries/zeta
    gpu: H100
    llm_server_config:
      tokenizer: Qwen/Qwen2.5-Coder-7B-Instruct
  - model: zed-industries/zeta
    gpu: H100
    llm_server_config:
      tokenizer: Qwen/Qwen2.5-Coder-7B-Instruct
      llm_kwargs:
        quant_config:
          quant_algo: FP8
  - model: meta-llama/Llama-3.3-70B-Instruct
    gpu: H100:4
    llm_server_config:
      llm_kwargs:
        tensor_parallel_size: 4
  - model: meta-llama/Llama-3.3-70B-Instruct
    gpu: H100:4
    llm_server_config:
      llm_kwargs:
        tensor_parallel_size: 4
        quant_config:
          quant_algo: FP8

  # Could not get DeepSeek-V3 to load, always OOMs
  # - model: cognitivecomputations/DeepSeek-V3-0324-AWQ
  #   gpu: H100:8
  #   llm_server_config:
  #     llm_kwargs:
  #       tensor_parallel_size: 8
  #       trust_remote_code: true
  #       kv_cache_config:
  #         free_gpu_memory_fraction: 0.05
  #       build_config:
  #         max_batch_size: 16
  #         max_num_tokens: 8192
  #       pytorch_backend_config:
  #         enable_overlap_scheduler: true
  #       backend: pytorch
  #       quant_config:
  #         quant_algo: MIXED_PRECISION
  #         kv_cache_quant_algo: FP8
  #     extra_args:
  #       - --backend
  #       - pytorch
