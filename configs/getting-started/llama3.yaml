id: llama3
base_config:
  model: meta-llama/Llama-3.1-8B-Instruct
  region: us-chicago-1
  llm_server_type: vllm
  data: prompt_tokens=1024,output_tokens=128
  gpu: H100!
