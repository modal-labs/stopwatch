id: llama3-data-configs
base_config:
  model: meta-llama/Llama-3.1-8B-Instruct
  region: us-chicago-1
  llm_server_type:
    - vllm
    - sglang
  data:
    - prompt_tokens=1024,output_tokens=128
    - prompt_tokens=128,output_tokens=1024
  gpu: H100!
