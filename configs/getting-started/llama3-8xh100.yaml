id: llama3-8xh100
base_config:
  model: meta-llama/Llama-3.1-70B-Instruct
  region: us-chicago-1
  data: prompt_tokens=1024,output_tokens=128
  gpu: H100!:8
configs:
  - llm_server_type: vllm
    llm_server_config:
      extra_args: ["--tensor-parallel-size", "8"]
  - llm_server_type: sglang
    llm_server_config:
      extra_args: ["--tp", "8"]
  - llm_server_type: tensorrt-llm
