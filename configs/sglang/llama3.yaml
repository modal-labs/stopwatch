id: sglang-llama3
base_config:
  region: us-chicago-1
  llm_server_type: sglang
  llm_server_config:
    extra_args: ["--chat-template", "/home/no-system-prompt.jinja"]
  data:
    - prompt_tokens=128,output_tokens=1024
    - prompt_tokens=1024,output_tokens=128
    - prompt_tokens=512,output_tokens=512
configs:
  - model: meta-llama/Llama-3.1-8B-Instruct
    gpu: H100
    llm_server_config:
      - extra_args: ["--chat-template", "/home/no-system-prompt.jinja"]
      - extra_args:
          [
            "--chat-template",
            "/home/no-system-prompt.jinja",
            "--quantization",
            "fp8",
          ]
  - model: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
    gpu: H100

  - model: meta-llama/Llama-3.1-70B-Instruct
    gpu: H100:8
    llm_server_config:
      - extra_args:
          ["--tp", "8", "--chat-template", "/home/no-system-prompt.jinja"]
      - extra_args:
          [
            "--tp",
            "8",
            "--chat-template",
            "/home/no-system-prompt.jinja",
            "--quantization",
            "fp8",
          ]
  - model: RedHatAI/Meta-Llama-3-70B-Instruct-quantized.w4a16
    gpu: H100:8
    llm_server_config:
      - extra_args:
          ["--tp", "8", "--chat-template", "/home/no-system-prompt.jinja"]
