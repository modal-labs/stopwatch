id: sglang
repeats: 3
base_config:
  region: us-chicago-1
  llm_server_type: sglang
  data:
    - prompt_tokens=256,output_tokens=4096
    - prompt_tokens=4096,output_tokens=256
    - prompt_tokens=2048,output_tokens=2048
configs:
  # Zeta benchmarks
  - model: zed-industries/zeta
    gpu: H100
    llm_server_config: # baseline
      tokenizer: Qwen/Qwen2.5-Coder-7B-Instruct
  - model: zed-industries/zeta
    gpu: H100
    llm_server_config: # latency-optimized
      tokenizer: Qwen/Qwen2.5-Coder-7B-Instruct
      extra_args:
        [
          "--enable-torch-compile",
          "--quantization",
          "fp8",
          "--kv-cache-dtype",
          "fp8_e5m2",
        ]
  - model: zed-industries/zeta
    gpu: H100
    data:
      - prompt_tokens=4096,output_tokens=256
      - prompt_tokens=2048,output_tokens=2048
    llm_server_config: # throughput-optimized
      tokenizer: Qwen/Qwen2.5-Coder-7B-Instruct
      extra_args:
        [
          "--enable-torch-compile",
          "--quantization",
          "fp8",
          "--kv-cache-dtype",
          "fp8_e5m2",
          "--stream-interval",
          "128",
        ]
  - model: zed-industries/zeta
    gpu: H100
    data: prompt_tokens=256,output_tokens=4096
    llm_server_config: # throughput-optimized for long generation
      tokenizer: Qwen/Qwen2.5-Coder-7B-Instruct
      extra_args: ["--stream-interval", "128"]

  # Llama-3.3-70B benchmarks
  - model: meta-llama/Llama-3.3-70B-Instruct
    gpu: H100:4
    llm_server_config:
      extra_args: ["--tp", "4", "--context-length", "8192"]
  - model: meta-llama/Llama-3.3-70B-Instruct
    gpu: H100:4
    llm_server_config: # latency-optimized
      extra_args:
        [
          "--tp",
          "4",
          "--context-length",
          "8192",
          "--enable-torch-compile",
          "--quantization",
          "fp8",
          "--kv-cache-dtype",
          "fp8_e5m2",
        ]
  - model: meta-llama/Llama-3.3-70B-Instruct
    gpu: H100:4
    llm_server_config: # throughput-optimized
      extra_args:
        [
          "--tp",
          "4",
          "--context-length",
          "8192",
          "--quantization",
          "fp8",
          "--stream-interval",
          "128",
        ]

  # DeepSeek-V3 benchmarks
  - model: cognitivecomputations/DeepSeek-V3-0324-AWQ
    gpu: H100:8
    llm_server_config:
      extra_args:
        [
          "--tp",
          "8",
          "--context-length",
          "8192",
          "--dtype",
          "half",
          "--trust-remote-code",
        ]
