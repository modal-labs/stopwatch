id: vllm-gpt-oss
base_config:
  region: us-chicago-1
  llm_server_type: vllm
  gpu: H100!
  data:
    - prompt_tokens=128,output_tokens=1024
    - prompt_tokens=1024,output_tokens=128
    - prompt_tokens=512,output_tokens=512
    - prompt_tokens=256,output_tokens=2048
    - prompt_tokens=2048,output_tokens=256
    - prompt_tokens=1024,output_tokens=1024
    - prompt_tokens=512,output_tokens=4096
    - prompt_tokens=4096,output_tokens=512
    - prompt_tokens=2048,output_tokens=2048
configs:
  - model: openai/gpt-oss-20b
  - model: openai/gpt-oss-120b
    llm_server_config:
      extra_args: ["--max-model-len", "16384"]
