id: vllm-llama3
base_config:
  region: us-chicago-1
  llm_server_type: vllm
  llm_server_config:
    extra_args: ["--chat-template", "/home/no-system-prompt.jinja"]
  data:
    - prompt_tokens=128,output_tokens=1024
    - prompt_tokens=1024,output_tokens=128
    - prompt_tokens=512,output_tokens=512
    - prompt_tokens=256,output_tokens=2048
    - prompt_tokens=2048,output_tokens=256
    - prompt_tokens=1024,output_tokens=1024
    - prompt_tokens=512,output_tokens=4096
    - prompt_tokens=4096,output_tokens=512
    - prompt_tokens=2048,output_tokens=2048
configs:
  - model: meta-llama/Llama-3.1-8B-Instruct
    gpu: H100
    llm_server_config:
      - extra_args: ["--chat-template", "/home/no-system-prompt.jinja"]
      - extra_args:
          [
            "--chat-template",
            "/home/no-system-prompt.jinja",
            "--quantization",
            "fp8",
          ]
  # - model: meta-llama/Llama-3.1-8B-Instruct
  #   gpu: A10
  #   region: us-ashburn-1
  #   llm_server_config:
  #     - extra_args:
  #         [
  #           "--chat-template",
  #           "/home/no-system-prompt.jinja",
  #           "--max-model-len",
  #           "8192",
  #         ]
  #     - extra_args:
  #         [
  #           "--chat-template",
  #           "/home/no-system-prompt.jinja",
  #           "--quantization",
  #           "fp8",
  #           "--max-model-len",
  #           "8192",
  #         ]
  # - model: meta-llama/Llama-3.1-8B-Instruct
  #   gpu: L40S
  #   region: us-ashburn-1
  #   llm_server_config:
  #     - extra_args: ["--chat-template", "/home/no-system-prompt.jinja"]
  #     - extra_args:
  #         [
  #           "--chat-template",
  #           "/home/no-system-prompt.jinja",
  #           "--quantization",
  #           "fp8",
  #         ]
  - model: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
    gpu: H100
  # - model: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
  #   gpu: A10
  #   region: us-ashburn-1
  #   llm_server_config:
  #     extra_args:
  #       [
  #         "--chat-template",
  #         "/home/no-system-prompt.jinja",
  #         "--max-model-len",
  #         "8192",
  #       ]
  # - model: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
  #   gpu: L40S
  #   region: us-ashburn-1

  - model: meta-llama/Llama-3.1-70B-Instruct
    gpu: H100:8
    llm_server_config:
      - extra_args:
          [
            "--tensor-parallel-size",
            "8",
            "--chat-template",
            "/home/no-system-prompt.jinja",
          ]
      - extra_args:
          [
            "--tensor-parallel-size",
            "8",
            "--chat-template",
            "/home/no-system-prompt.jinja",
            "--quantization",
            "fp8",
          ]
  # - model: meta-llama/Llama-3.1-70B-Instruct
  #   gpu: L40S:4
  #   region: us-ashburn-1
  #   llm_server_config:
  #     - extra_args:
  #         [
  #           "--tensor-parallel-size",
  #           "4",
  #           "--chat-template",
  #           "/home/no-system-prompt.jinja",
  #           "--max-model-len",
  #           "8192",
  #         ]
  #     - extra_args:
  #         [
  #           "--tensor-parallel-size",
  #           "4",
  #           "--chat-template",
  #           "/home/no-system-prompt.jinja",
  #           "--quantization",
  #           "fp8",
  #           "--max-model-len",
  #           "8192",
  #         ]
  - model: RedHatAI/Meta-Llama-3-70B-Instruct-quantized.w4a16
    gpu: H100:8
    llm_server_config:
      extra_args:
        [
          "--tensor-parallel-size",
          "8",
          "--chat-template",
          "/home/no-system-prompt.jinja",
        ]
  # - model: RedHatAI/Meta-Llama-3-70B-Instruct-quantized.w4a16
  #   gpu: L40S:4
  #   region: us-ashburn-1
  #   llm_server_config:
  #     extra_args:
  #       [
  #         "--tensor-parallel-size",
  #         "4",
  #         "--chat-template",
  #         "/home/no-system-prompt.jinja",
  #         "--max-model-len",
  #         "8192",
  #       ]
