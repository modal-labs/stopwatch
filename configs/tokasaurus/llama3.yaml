id: tokasaurus-llama3
base_config:
  region: us-chicago-1
  llm_server_type: tokasaurus
  data:
    - prompt_tokens=128,output_tokens=1024
    - prompt_tokens=1024,output_tokens=128
    - prompt_tokens=512,output_tokens=512
    - prompt_tokens=256,output_tokens=2048
    - prompt_tokens=2048,output_tokens=256
    - prompt_tokens=1024,output_tokens=1024
    - prompt_tokens=512,output_tokens=4096
    - prompt_tokens=4096,output_tokens=512
    - prompt_tokens=2048,output_tokens=2048
configs:
  - model: meta-llama/Llama-3.1-8B-Instruct
    gpu: H100!
    llm_server_config:
      extra_args: ["torch_compile=T"]
  - model: meta-llama/Llama-3.1-70B-Instruct
    gpu: H100!:8
    llm_server_config:
      extra_args: ["kv_cache_num_tokens=524288", "pp_size=8", "torch_compile=T"]
